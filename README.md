# Machine Translation using seq2seq model and attention mechanisms

### Description:
This project by Th√©o Gachet aims to automate French-to-English text translation using Seq2Seq with attention mechanisms, covering data preparation, tokenization, text vectorization, and model training. The goal is to build an efficient and accurate translation pipeline that understands French sentences and generates high-quality English translations. The Seq2Seq architecture, along with attention mechanisms, enables the model to capture language patterns and context effectively, leading to improved translation quality. The outcome is a powerful tool for automatic translation, contributing to advancements in natural language processing.

### Table of Contents:
1. [Prerequisites](#prerequisites)
2. [Installation](#installation)
3. [Usage](#usage)
4. [Code Structure](#code-structure)
5. [Contribute](#contribute)
6. [License](#license)

### Prerequisites:
- Python 3.8 or higher
- TensorFlow 2.x and TensorFlow Text libraries

### Installation:
1. Clone this repository:
```
git clone repository_link.git
cd repository_directory
```

2. Install the required dependencies:
```
pip install -r requirements.txt
```

### Usage:
Run the main script to initiate the translation process:
```
python main_script_name.py
```

### Code Structure:
1. **Training and Validation Datasets**:
   - Define parameters for data preparation.
   - Generate training and validation datasets.
   - Display a few sample instances for verification.

2. **Preprocessing**:
   - **Text Standardization**:
     - Transform the text examples.
     - Normalize text to remove variations caused by casing or special characters.
     - Text transformation: making lowercase and separating out punctuation.

   - **Vectorization**:
     - Define a maximum vocabulary size.
     - Create a text processor for the source (English).
     - Adapt the processor to training data.
     - Create a text processor for the target (French).
     - Examples of tokens generated by the processor.
     - Convert tokens back into words.
     - Display a visual representation of token IDs and their mask.
     - Process text before feeding into the model: converting source and target into tokens.

3. **Seq2Seq Model with Attention**:
   - Define parameters for the model.
   - Create encoder and decoder layers.
   - Implement the attention mechanism to focus on specific parts of the input while generating output.
   - Compile, train, and validate the model.
   - Display sample translations.

### Contribute:
If you're interested in contributing to this project, kindly follow these steps:
1. Fork the repository.
2. Create your feature branch (`git checkout -b feature/my-feature`).
3. Commit your changes (`git commit -am 'Added my feature'`).
4. Push to the branch (`git push origin feature/my-feature`).
5. Open a Pull Request.

### License:
This project is licensed under the MIT License. For more details, please see the [LICENSE.md](LICENSE.md) file.

---

*Feel free to report any issues or suggest new features. Your feedback is highly appreciated!*
